{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LeakyReLU, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from contrastive import CPCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E001h3k4me3</th>\n",
       "      <th>E002h3k4me3</th>\n",
       "      <th>E003h3k4me3</th>\n",
       "      <th>E004h3k4me3</th>\n",
       "      <th>E005h3k4me3</th>\n",
       "      <th>E006h3k4me3</th>\n",
       "      <th>E007h3k4me3</th>\n",
       "      <th>E008h3k4me3</th>\n",
       "      <th>E009h3k4me3</th>\n",
       "      <th>E010h3k4me3</th>\n",
       "      <th>...</th>\n",
       "      <th>E056tensor</th>\n",
       "      <th>E059tensor</th>\n",
       "      <th>E089tensor</th>\n",
       "      <th>E090tensor</th>\n",
       "      <th>E094tensor</th>\n",
       "      <th>E097tensor</th>\n",
       "      <th>E098tensor</th>\n",
       "      <th>E100tensor</th>\n",
       "      <th>E109tensor</th>\n",
       "      <th>epit.entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000004866</th>\n",
       "      <td>0.296625</td>\n",
       "      <td>0.105981</td>\n",
       "      <td>0.249337</td>\n",
       "      <td>0.184611</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>-0.103241</td>\n",
       "      <td>-0.254451</td>\n",
       "      <td>-0.153236</td>\n",
       "      <td>0.567978</td>\n",
       "      <td>2.079856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525170</td>\n",
       "      <td>1.154039</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>0.718738</td>\n",
       "      <td>0.699267</td>\n",
       "      <td>0.061643</td>\n",
       "      <td>1.580294</td>\n",
       "      <td>2.929246</td>\n",
       "      <td>0.954994</td>\n",
       "      <td>0.658541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000005339</th>\n",
       "      <td>0.664484</td>\n",
       "      <td>0.469639</td>\n",
       "      <td>0.848853</td>\n",
       "      <td>1.325303</td>\n",
       "      <td>0.711186</td>\n",
       "      <td>0.610536</td>\n",
       "      <td>-0.522232</td>\n",
       "      <td>0.906287</td>\n",
       "      <td>0.822634</td>\n",
       "      <td>1.136299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499322</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.400658</td>\n",
       "      <td>-0.011319</td>\n",
       "      <td>0.565993</td>\n",
       "      <td>0.335474</td>\n",
       "      <td>0.384709</td>\n",
       "      <td>-0.146969</td>\n",
       "      <td>-0.043804</td>\n",
       "      <td>0.821583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000005513</th>\n",
       "      <td>0.204398</td>\n",
       "      <td>1.045012</td>\n",
       "      <td>0.147311</td>\n",
       "      <td>-0.064777</td>\n",
       "      <td>1.585373</td>\n",
       "      <td>2.315699</td>\n",
       "      <td>-0.443473</td>\n",
       "      <td>0.536371</td>\n",
       "      <td>1.998647</td>\n",
       "      <td>0.870081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411337</td>\n",
       "      <td>0.642630</td>\n",
       "      <td>0.706508</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>0.832542</td>\n",
       "      <td>0.198559</td>\n",
       "      <td>-0.158738</td>\n",
       "      <td>-0.293455</td>\n",
       "      <td>-0.614546</td>\n",
       "      <td>0.555202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000006704</th>\n",
       "      <td>-0.035464</td>\n",
       "      <td>0.227201</td>\n",
       "      <td>0.362198</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>1.144966</td>\n",
       "      <td>1.109917</td>\n",
       "      <td>1.588511</td>\n",
       "      <td>1.286247</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.437056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>-0.252335</td>\n",
       "      <td>0.298708</td>\n",
       "      <td>0.079938</td>\n",
       "      <td>-0.233652</td>\n",
       "      <td>-0.075272</td>\n",
       "      <td>-0.267428</td>\n",
       "      <td>-0.439942</td>\n",
       "      <td>-0.186490</td>\n",
       "      <td>1.063549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000007168</th>\n",
       "      <td>0.896981</td>\n",
       "      <td>0.367413</td>\n",
       "      <td>0.612749</td>\n",
       "      <td>1.201246</td>\n",
       "      <td>0.086256</td>\n",
       "      <td>0.387711</td>\n",
       "      <td>0.454905</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>1.162038</td>\n",
       "      <td>1.392407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752834</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.910407</td>\n",
       "      <td>0.536223</td>\n",
       "      <td>0.965816</td>\n",
       "      <td>1.156964</td>\n",
       "      <td>0.710778</td>\n",
       "      <td>0.878436</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>0.392160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 E001h3k4me3  E002h3k4me3  E003h3k4me3  E004h3k4me3  \\\n",
       "ENSG00000004866     0.296625     0.105981     0.249337     0.184611   \n",
       "ENSG00000005339     0.664484     0.469639     0.848853     1.325303   \n",
       "ENSG00000005513     0.204398     1.045012     0.147311    -0.064777   \n",
       "ENSG00000006704    -0.035464     0.227201     0.362198     0.396463   \n",
       "ENSG00000007168     0.896981     0.367413     0.612749     1.201246   \n",
       "\n",
       "                 E005h3k4me3  E006h3k4me3  E007h3k4me3  E008h3k4me3  \\\n",
       "ENSG00000004866     0.007758    -0.103241    -0.254451    -0.153236   \n",
       "ENSG00000005339     0.711186     0.610536    -0.522232     0.906287   \n",
       "ENSG00000005513     1.585373     2.315699    -0.443473     0.536371   \n",
       "ENSG00000006704     1.144966     1.109917     1.588511     1.286247   \n",
       "ENSG00000007168     0.086256     0.387711     0.454905     0.145055   \n",
       "\n",
       "                 E009h3k4me3  E010h3k4me3      ...       E056tensor  \\\n",
       "ENSG00000004866     0.567978     2.079856      ...         0.525170   \n",
       "ENSG00000005339     0.822634     1.136299      ...        -0.499322   \n",
       "ENSG00000005513     1.998647     0.870081      ...         0.411337   \n",
       "ENSG00000006704     0.893187     0.437056      ...         0.069840   \n",
       "ENSG00000007168     1.162038     1.392407      ...         0.752834   \n",
       "\n",
       "                 E059tensor  E089tensor  E090tensor  E094tensor  E097tensor  \\\n",
       "ENSG00000004866    1.154039    0.094809    0.718738    0.699267    0.061643   \n",
       "ENSG00000005339    0.003369    0.400658   -0.011319    0.565993    0.335474   \n",
       "ENSG00000005513    0.642630    0.706508    0.171195    0.832542    0.198559   \n",
       "ENSG00000006704   -0.252335    0.298708    0.079938   -0.233652   -0.075272   \n",
       "ENSG00000007168    0.898335    0.910407    0.536223    0.965816    1.156964   \n",
       "\n",
       "                 E098tensor  E100tensor  E109tensor  epit.entropy  \n",
       "ENSG00000004866    1.580294    2.929246    0.954994      0.658541  \n",
       "ENSG00000005339    0.384709   -0.146969   -0.043804      0.821583  \n",
       "ENSG00000005513   -0.158738   -0.293455   -0.614546      0.555202  \n",
       "ENSG00000006704   -0.267428   -0.439942   -0.186490      1.063549  \n",
       "ENSG00000007168    0.710778    0.878436    0.812308      0.392160  \n",
       "\n",
       "[5 rows x 359 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/all_genes_norm.csv\", index_col = 0)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19430, 359)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E001h3k4me3</th>\n",
       "      <th>E002h3k4me3</th>\n",
       "      <th>E003h3k4me3</th>\n",
       "      <th>E004h3k4me3</th>\n",
       "      <th>E005h3k4me3</th>\n",
       "      <th>E006h3k4me3</th>\n",
       "      <th>E007h3k4me3</th>\n",
       "      <th>E008h3k4me3</th>\n",
       "      <th>E009h3k4me3</th>\n",
       "      <th>E010h3k4me3</th>\n",
       "      <th>...</th>\n",
       "      <th>E056tensor</th>\n",
       "      <th>E059tensor</th>\n",
       "      <th>E089tensor</th>\n",
       "      <th>E090tensor</th>\n",
       "      <th>E094tensor</th>\n",
       "      <th>E097tensor</th>\n",
       "      <th>E098tensor</th>\n",
       "      <th>E100tensor</th>\n",
       "      <th>E109tensor</th>\n",
       "      <th>epit.entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000115221</th>\n",
       "      <td>-1.203563</td>\n",
       "      <td>-0.136458</td>\n",
       "      <td>-1.214242</td>\n",
       "      <td>-1.136124</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>-1.109903</td>\n",
       "      <td>-0.954356</td>\n",
       "      <td>-1.228917</td>\n",
       "      <td>-1.171202</td>\n",
       "      <td>-1.145929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840819</td>\n",
       "      <td>-0.891597</td>\n",
       "      <td>-0.822740</td>\n",
       "      <td>-0.923890</td>\n",
       "      <td>-0.900022</td>\n",
       "      <td>-0.896762</td>\n",
       "      <td>-0.919565</td>\n",
       "      <td>-0.879401</td>\n",
       "      <td>-0.899917</td>\n",
       "      <td>-1.477453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000117410</th>\n",
       "      <td>-0.357033</td>\n",
       "      <td>-0.271642</td>\n",
       "      <td>0.447520</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.473651</td>\n",
       "      <td>0.164360</td>\n",
       "      <td>0.711135</td>\n",
       "      <td>4.023710</td>\n",
       "      <td>2.602726</td>\n",
       "      <td>-0.027983</td>\n",
       "      <td>...</td>\n",
       "      <td>1.435829</td>\n",
       "      <td>2.049005</td>\n",
       "      <td>2.235755</td>\n",
       "      <td>2.270108</td>\n",
       "      <td>1.632186</td>\n",
       "      <td>1.704624</td>\n",
       "      <td>1.471604</td>\n",
       "      <td>2.343300</td>\n",
       "      <td>1.525735</td>\n",
       "      <td>0.693065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000249115</th>\n",
       "      <td>-0.560074</td>\n",
       "      <td>-0.362138</td>\n",
       "      <td>-0.536626</td>\n",
       "      <td>-0.442038</td>\n",
       "      <td>1.829024</td>\n",
       "      <td>-0.386118</td>\n",
       "      <td>0.250657</td>\n",
       "      <td>0.117977</td>\n",
       "      <td>-0.010636</td>\n",
       "      <td>0.048260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840819</td>\n",
       "      <td>-0.891597</td>\n",
       "      <td>-0.822740</td>\n",
       "      <td>-0.923890</td>\n",
       "      <td>-0.900022</td>\n",
       "      <td>-0.896762</td>\n",
       "      <td>-0.919565</td>\n",
       "      <td>-0.879401</td>\n",
       "      <td>-0.899917</td>\n",
       "      <td>-1.477453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000063761</th>\n",
       "      <td>-0.683512</td>\n",
       "      <td>-0.464364</td>\n",
       "      <td>-0.288332</td>\n",
       "      <td>0.334752</td>\n",
       "      <td>-0.646736</td>\n",
       "      <td>-0.493579</td>\n",
       "      <td>-0.485478</td>\n",
       "      <td>-0.522278</td>\n",
       "      <td>-0.466930</td>\n",
       "      <td>-0.461429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297505</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.298708</td>\n",
       "      <td>0.536223</td>\n",
       "      <td>1.099090</td>\n",
       "      <td>0.883134</td>\n",
       "      <td>1.145536</td>\n",
       "      <td>1.171409</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>0.688392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000135047</th>\n",
       "      <td>-0.291457</td>\n",
       "      <td>0.270773</td>\n",
       "      <td>-0.362369</td>\n",
       "      <td>-0.095950</td>\n",
       "      <td>-0.109480</td>\n",
       "      <td>0.154878</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>-0.356318</td>\n",
       "      <td>0.331272</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297505</td>\n",
       "      <td>1.281892</td>\n",
       "      <td>0.298708</td>\n",
       "      <td>0.353709</td>\n",
       "      <td>0.699267</td>\n",
       "      <td>1.430794</td>\n",
       "      <td>-0.919565</td>\n",
       "      <td>-0.000483</td>\n",
       "      <td>-0.186490</td>\n",
       "      <td>0.826255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 E001h3k4me3  E002h3k4me3  E003h3k4me3  E004h3k4me3  \\\n",
       "ENSG00000115221    -1.203563    -0.136458    -1.214242    -1.136124   \n",
       "ENSG00000117410    -0.357033    -0.271642     0.447520     0.082184   \n",
       "ENSG00000249115    -0.560074    -0.362138    -0.536626    -0.442038   \n",
       "ENSG00000063761    -0.683512    -0.464364    -0.288332     0.334752   \n",
       "ENSG00000135047    -0.291457     0.270773    -0.362369    -0.095950   \n",
       "\n",
       "                 E005h3k4me3  E006h3k4me3  E007h3k4me3  E008h3k4me3  \\\n",
       "ENSG00000115221     0.233568    -1.109903    -0.954356    -1.228917   \n",
       "ENSG00000117410     0.473651     0.164360     0.711135     4.023710   \n",
       "ENSG00000249115     1.829024    -0.386118     0.250657     0.117977   \n",
       "ENSG00000063761    -0.646736    -0.493579    -0.485478    -0.522278   \n",
       "ENSG00000135047    -0.109480     0.154878     0.015430    -0.356318   \n",
       "\n",
       "                 E009h3k4me3  E010h3k4me3      ...       E056tensor  \\\n",
       "ENSG00000115221    -1.171202    -1.145929      ...        -0.840819   \n",
       "ENSG00000117410     2.602726    -0.027983      ...         1.435829   \n",
       "ENSG00000249115    -0.010636     0.048260      ...        -0.840819   \n",
       "ENSG00000063761    -0.466930    -0.461429      ...         0.297505   \n",
       "ENSG00000135047     0.331272    -0.168252      ...         0.297505   \n",
       "\n",
       "                 E059tensor  E089tensor  E090tensor  E094tensor  E097tensor  \\\n",
       "ENSG00000115221   -0.891597   -0.822740   -0.923890   -0.900022   -0.896762   \n",
       "ENSG00000117410    2.049005    2.235755    2.270108    1.632186    1.704624   \n",
       "ENSG00000249115   -0.891597   -0.822740   -0.923890   -0.900022   -0.896762   \n",
       "ENSG00000063761    0.898335    0.298708    0.536223    1.099090    0.883134   \n",
       "ENSG00000135047    1.281892    0.298708    0.353709    0.699267    1.430794   \n",
       "\n",
       "                 E098tensor  E100tensor  E109tensor  epit.entropy  \n",
       "ENSG00000115221   -0.919565   -0.879401   -0.899917     -1.477453  \n",
       "ENSG00000117410    1.471604    2.343300    1.525735      0.693065  \n",
       "ENSG00000249115   -0.919565   -0.879401   -0.899917     -1.477453  \n",
       "ENSG00000063761    1.145536    1.171409    0.812308      0.688392  \n",
       "ENSG00000135047   -0.919565   -0.000483   -0.186490      0.826255  \n",
       "\n",
       "[5 rows x 359 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/2000_sample_norm.csv\", index_col = 0)\n",
    "X_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 359)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19430 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "19430/19430 [==============================] - 2s 90us/step - loss: 0.4075 - val_loss: 0.2843\n",
      "Epoch 2/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.2477 - val_loss: 0.2224\n",
      "Epoch 3/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.2055 - val_loss: 0.1957\n",
      "Epoch 4/100\n",
      "19430/19430 [==============================] - 1s 75us/step - loss: 0.1872 - val_loss: 0.1842\n",
      "Epoch 5/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1762 - val_loss: 0.1738\n",
      "Epoch 6/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1704 - val_loss: 0.1678\n",
      "Epoch 7/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1650 - val_loss: 0.1623\n",
      "Epoch 8/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1618 - val_loss: 0.1614\n",
      "Epoch 9/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1609 - val_loss: 0.1586\n",
      "Epoch 10/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1594 - val_loss: 0.1565\n",
      "Epoch 11/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1564 - val_loss: 0.1553\n",
      "Epoch 12/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1548 - val_loss: 0.1535\n",
      "Epoch 13/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1542 - val_loss: 0.1528\n",
      "Epoch 14/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1529 - val_loss: 0.1524\n",
      "Epoch 15/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.1527 - val_loss: 0.1526\n",
      "Epoch 16/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1517 - val_loss: 0.1508\n",
      "Epoch 17/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1499 - val_loss: 0.1479\n",
      "Epoch 18/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1496 - val_loss: 0.1487\n",
      "Epoch 19/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1491 - val_loss: 0.1488\n",
      "Epoch 20/100\n",
      "19430/19430 [==============================] - 1s 65us/step - loss: 0.1486 - val_loss: 0.1472\n",
      "Epoch 21/100\n",
      "19430/19430 [==============================] - 1s 60us/step - loss: 0.1490 - val_loss: 0.1472\n",
      "Epoch 22/100\n",
      "19430/19430 [==============================] - 1s 60us/step - loss: 0.1474 - val_loss: 0.1463\n",
      "Epoch 23/100\n",
      "19430/19430 [==============================] - 1s 60us/step - loss: 0.1470 - val_loss: 0.1472\n",
      "Epoch 24/100\n",
      "19430/19430 [==============================] - 1s 60us/step - loss: 0.1467 - val_loss: 0.1461\n",
      "Epoch 25/100\n",
      "19430/19430 [==============================] - 1s 62us/step - loss: 0.1459 - val_loss: 0.1451\n",
      "Epoch 26/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1460 - val_loss: 0.1520\n",
      "Epoch 27/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1465 - val_loss: 0.1438\n",
      "Epoch 28/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1443 - val_loss: 0.1429\n",
      "Epoch 29/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1435 - val_loss: 0.1437\n",
      "Epoch 30/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.1436 - val_loss: 0.1424\n",
      "Epoch 31/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.1433 - val_loss: 0.1439\n",
      "Epoch 32/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1431 - val_loss: 0.1509\n",
      "Epoch 33/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1428 - val_loss: 0.1437\n",
      "Epoch 34/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1427 - val_loss: 0.1408\n",
      "Epoch 35/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.1419 - val_loss: 0.1412\n",
      "Epoch 36/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1414 - val_loss: 0.1404\n",
      "Epoch 37/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1415 - val_loss: 0.1437\n",
      "Epoch 38/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1417 - val_loss: 0.1409\n",
      "Epoch 39/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1409 - val_loss: 0.1408\n",
      "Epoch 40/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1402 - val_loss: 0.1412\n",
      "Epoch 41/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1400 - val_loss: 0.1393\n",
      "Epoch 42/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1401 - val_loss: 0.1408\n",
      "Epoch 43/100\n",
      "19430/19430 [==============================] - 1s 69us/step - loss: 0.1397 - val_loss: 0.1409\n",
      "Epoch 44/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1404 - val_loss: 0.1390\n",
      "Epoch 45/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1398 - val_loss: 0.1418\n",
      "Epoch 46/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1393 - val_loss: 0.1379\n",
      "Epoch 47/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1384 - val_loss: 0.1389\n",
      "Epoch 48/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1383 - val_loss: 0.1374\n",
      "Epoch 49/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1379 - val_loss: 0.1371\n",
      "Epoch 50/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1379 - val_loss: 0.1371\n",
      "Epoch 51/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1381 - val_loss: 0.1383\n",
      "Epoch 52/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1381 - val_loss: 0.1376\n",
      "Epoch 53/100\n",
      "19430/19430 [==============================] - 1s 70us/step - loss: 0.1374 - val_loss: 0.1361\n",
      "Epoch 54/100\n",
      "19430/19430 [==============================] - 1s 72us/step - loss: 0.1365 - val_loss: 0.1375\n",
      "Epoch 55/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1370 - val_loss: 0.1371\n",
      "Epoch 56/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1368 - val_loss: 0.1369\n",
      "Epoch 57/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1369 - val_loss: 0.1388\n",
      "Epoch 58/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1370 - val_loss: 0.1372\n",
      "Epoch 59/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1361 - val_loss: 0.1352\n",
      "Epoch 60/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1361 - val_loss: 0.1352\n",
      "Epoch 61/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1356 - val_loss: 0.1356\n",
      "Epoch 62/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1360 - val_loss: 0.1355\n",
      "Epoch 63/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1358 - val_loss: 0.1352\n",
      "Epoch 64/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1355 - val_loss: 0.1337\n",
      "Epoch 65/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1357 - val_loss: 0.1356\n",
      "Epoch 66/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1348 - val_loss: 0.1345\n",
      "Epoch 67/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1346 - val_loss: 0.1348\n",
      "Epoch 68/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1347 - val_loss: 0.1356\n",
      "Epoch 69/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1356 - val_loss: 0.1343\n",
      "Epoch 70/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1345 - val_loss: 0.1339\n",
      "Epoch 71/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1346 - val_loss: 0.1343\n",
      "Epoch 72/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1344 - val_loss: 0.1335\n",
      "Epoch 73/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1347 - val_loss: 0.1344\n",
      "Epoch 74/100\n",
      "19430/19430 [==============================] - 2s 80us/step - loss: 0.1340 - val_loss: 0.1331\n",
      "Epoch 75/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1338 - val_loss: 0.1331\n",
      "Epoch 76/100\n",
      "19430/19430 [==============================] - 1s 68us/step - loss: 0.1340 - val_loss: 0.1352\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1339 - val_loss: 0.1334\n",
      "Epoch 78/100\n",
      "19430/19430 [==============================] - 1s 66us/step - loss: 0.1334 - val_loss: 0.1334\n",
      "Epoch 79/100\n",
      "19430/19430 [==============================] - 1s 66us/step - loss: 0.1334 - val_loss: 0.1323\n",
      "Epoch 80/100\n",
      "19430/19430 [==============================] - 1s 74us/step - loss: 0.1330 - val_loss: 0.1336\n",
      "Epoch 81/100\n",
      "19430/19430 [==============================] - 2s 86us/step - loss: 0.1337 - val_loss: 0.1348\n",
      "Epoch 82/100\n",
      "19430/19430 [==============================] - 2s 86us/step - loss: 0.1339 - val_loss: 0.1333\n",
      "Epoch 83/100\n",
      "19430/19430 [==============================] - 2s 100us/step - loss: 0.1329 - val_loss: 0.1334\n",
      "Epoch 84/100\n",
      "19430/19430 [==============================] - 2s 77us/step - loss: 0.1329 - val_loss: 0.1316\n",
      "Epoch 85/100\n",
      "19430/19430 [==============================] - 1s 73us/step - loss: 0.1330 - val_loss: 0.1343\n",
      "Epoch 86/100\n",
      "19430/19430 [==============================] - 1s 75us/step - loss: 0.1328 - val_loss: 0.1319\n",
      "Epoch 87/100\n",
      "19430/19430 [==============================] - 2s 79us/step - loss: 0.1325 - val_loss: 0.1324\n",
      "Epoch 88/100\n",
      "19430/19430 [==============================] - 1s 71us/step - loss: 0.1329 - val_loss: 0.1318\n",
      "Epoch 89/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1331 - val_loss: 0.1315\n",
      "Epoch 90/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1325 - val_loss: 0.1317\n",
      "Epoch 91/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1322 - val_loss: 0.1322\n",
      "Epoch 92/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1325 - val_loss: 0.1327\n",
      "Epoch 93/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1322 - val_loss: 0.1311\n",
      "Epoch 94/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1318 - val_loss: 0.1306\n",
      "Epoch 95/100\n",
      "19430/19430 [==============================] - 1s 67us/step - loss: 0.1320 - val_loss: 0.1322\n",
      "Epoch 96/100\n",
      "19430/19430 [==============================] - 1s 75us/step - loss: 0.1322 - val_loss: 0.1326\n",
      "Epoch 97/100\n",
      "19430/19430 [==============================] - 2s 78us/step - loss: 0.1317 - val_loss: 0.1311\n",
      "Epoch 98/100\n",
      "19430/19430 [==============================] - 1s 73us/step - loss: 0.1325 - val_loss: 0.1314\n",
      "Epoch 99/100\n",
      "19430/19430 [==============================] - 1s 75us/step - loss: 0.1317 - val_loss: 0.1308\n",
      "Epoch 100/100\n",
      "19430/19430 [==============================] - 2s 86us/step - loss: 0.1317 - val_loss: 0.1317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24f20be0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1 = Sequential()\n",
    "Model1.add(Dense(100,input_shape = (359,),name = 'inter1'))\n",
    "Model1.add(LeakyReLU(alpha=0.3))\n",
    "Model1.add(Dense(359,))\n",
    "Model1.add(LeakyReLU(alpha=0.3))\n",
    "\n",
    "Model1.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "Model1.fit(X.values, X.values,\n",
    "           epochs = 100, \n",
    "           batch_size = 64,\n",
    "           validation_data = (X_sample.values,X_sample.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle1 = Model(inputs = Model1.input,outputs = Model1.get_layer('inter1').output)\n",
    "middle1_out = middle1.predict(X.values)\n",
    "middle1_out_vld = middle1.predict(X_sample.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.31379032,   6.85312843,   5.65701246,   8.59158421,\n",
       "        10.3438158 ,  12.9912653 ,   8.90900421,   6.21373749,\n",
       "         6.20271063,  10.06014633,   8.68266487,   7.02023792,\n",
       "        17.29282379,  11.46195316,   4.85800457,   6.74078369,\n",
       "        11.59032631,   9.68308449,   5.98702621,   6.825037  ,\n",
       "         6.18343496,  18.3867321 ,   6.38755894,   4.61319256,\n",
       "        10.99646854,   4.64433479,   9.92975426,   5.2706399 ,\n",
       "         8.80031776,  13.14039421,  10.10639858,   6.12761402,\n",
       "        10.51253986,   9.55984783,  10.3263607 ,  10.20404625,\n",
       "        12.51140976,  10.01408482,   7.32546425,   5.14242268,\n",
       "         8.38302231,   5.9806695 ,  11.33290958,  11.40169907,\n",
       "        12.86963177,   8.60473537,  10.59547806,  12.3857069 ,\n",
       "         3.00383997,   6.82080126,  10.66193676,   3.81934476,\n",
       "         4.05250835,   8.48815632,   7.71562099,   5.83293343,\n",
       "         6.56346464,  15.45814991,  19.89453888,   6.45917845,\n",
       "         5.56066561,   8.0441227 ,   8.47682667,   5.67606735,\n",
       "         7.37988186,   7.67775488,   6.90175915,  14.60668087,\n",
       "         8.6758213 ,  24.90570259,   5.97526979,  13.3759222 ,\n",
       "        13.47929955,   8.48501682,   7.5114646 ,   4.48849678,\n",
       "         9.38487625,  11.56276894,   5.98650837,  10.1918602 ,\n",
       "         4.74257565,  12.91227245,  16.24438858,  13.30369663,\n",
       "         8.54983997,   9.90987015,   8.62248325,   8.55603886,\n",
       "         9.05497074,  15.50804138,  13.41645622,  11.82439804,\n",
       "         6.59268999,   5.47986603,   7.63839149,   7.25470352,\n",
       "        13.16001225,  16.50062943,  10.99555779,  12.18733597], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(middle1_out, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19430 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "19430/19430 [==============================] - 2s 80us/step - loss: 3.2535 - val_loss: 2.2494\n",
      "Epoch 2/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 1.8474 - val_loss: 1.6063\n",
      "Epoch 3/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 1.4261 - val_loss: 1.3159\n",
      "Epoch 4/100\n",
      "19430/19430 [==============================] - ETA: 0s - loss: 1.215 - 1s 29us/step - loss: 1.2117 - val_loss: 1.1435\n",
      "Epoch 5/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 1.0774 - val_loss: 1.0286\n",
      "Epoch 6/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.9887 - val_loss: 0.9639\n",
      "Epoch 7/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 0.9291 - val_loss: 0.9089\n",
      "Epoch 8/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 0.8872 - val_loss: 0.8654\n",
      "Epoch 9/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.8559 - val_loss: 0.8458\n",
      "Epoch 10/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.8303 - val_loss: 0.8287\n",
      "Epoch 11/100\n",
      "19430/19430 [==============================] - 1s 36us/step - loss: 0.8108 - val_loss: 0.8071\n",
      "Epoch 12/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.7941 - val_loss: 0.7918\n",
      "Epoch 13/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.7797 - val_loss: 0.7806\n",
      "Epoch 14/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 0.7680 - val_loss: 0.7715\n",
      "Epoch 15/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.7565 - val_loss: 0.7671\n",
      "Epoch 16/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.7488 - val_loss: 0.7475\n",
      "Epoch 17/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.7400 - val_loss: 0.7519\n",
      "Epoch 18/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.7333 - val_loss: 0.7345\n",
      "Epoch 19/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.7263 - val_loss: 0.7287\n",
      "Epoch 20/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.7214 - val_loss: 0.7230\n",
      "Epoch 21/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.7170 - val_loss: 0.7098\n",
      "Epoch 22/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 0.7120 - val_loss: 0.7086\n",
      "Epoch 23/100\n",
      "19430/19430 [==============================] - 1s 45us/step - loss: 0.7067 - val_loss: 0.7041\n",
      "Epoch 24/100\n",
      "19430/19430 [==============================] - 1s 39us/step - loss: 0.7032 - val_loss: 0.6993\n",
      "Epoch 25/100\n",
      "19430/19430 [==============================] - 1s 35us/step - loss: 0.6998 - val_loss: 0.6984\n",
      "Epoch 26/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6960 - val_loss: 0.6999\n",
      "Epoch 27/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6948 - val_loss: 0.7002\n",
      "Epoch 28/100\n",
      "19430/19430 [==============================] - 1s 40us/step - loss: 0.6922 - val_loss: 0.6922\n",
      "Epoch 29/100\n",
      "19430/19430 [==============================] - 1s 38us/step - loss: 0.6898 - val_loss: 0.6960\n",
      "Epoch 30/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 0.6871 - val_loss: 0.6871\n",
      "Epoch 31/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6868 - val_loss: 0.6938\n",
      "Epoch 32/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.6834 - val_loss: 0.6808\n",
      "Epoch 33/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6810 - val_loss: 0.6833\n",
      "Epoch 34/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 0.6806 - val_loss: 0.6772\n",
      "Epoch 35/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6792 - val_loss: 0.6762\n",
      "Epoch 36/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6777 - val_loss: 0.6881\n",
      "Epoch 37/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6768 - val_loss: 0.6739\n",
      "Epoch 38/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6745 - val_loss: 0.6936\n",
      "Epoch 39/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6746 - val_loss: 0.6729\n",
      "Epoch 40/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6731 - val_loss: 0.6704\n",
      "Epoch 41/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6727 - val_loss: 0.6679\n",
      "Epoch 42/100\n",
      "19430/19430 [==============================] - 1s 34us/step - loss: 0.6726 - val_loss: 0.6701\n",
      "Epoch 43/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6697 - val_loss: 0.6672\n",
      "Epoch 44/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6684 - val_loss: 0.6667\n",
      "Epoch 45/100\n",
      "19430/19430 [==============================] - 1s 35us/step - loss: 0.6692 - val_loss: 0.6680\n",
      "Epoch 46/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6673 - val_loss: 0.6691\n",
      "Epoch 47/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6669 - val_loss: 0.6688\n",
      "Epoch 48/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 0.6651 - val_loss: 0.6698\n",
      "Epoch 49/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6654 - val_loss: 0.6667\n",
      "Epoch 50/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6647 - val_loss: 0.6632\n",
      "Epoch 51/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6639 - val_loss: 0.6661\n",
      "Epoch 52/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6632 - val_loss: 0.6601\n",
      "Epoch 53/100\n",
      "19430/19430 [==============================] - 1s 50us/step - loss: 0.6617 - val_loss: 0.6586\n",
      "Epoch 54/100\n",
      "19430/19430 [==============================] - 1s 33us/step - loss: 0.6610 - val_loss: 0.6632\n",
      "Epoch 55/100\n",
      "19430/19430 [==============================] - 1s 40us/step - loss: 0.6601 - val_loss: 0.6630\n",
      "Epoch 56/100\n",
      "19430/19430 [==============================] - 1s 43us/step - loss: 0.6597 - val_loss: 0.6579\n",
      "Epoch 57/100\n",
      "19430/19430 [==============================] - 1s 36us/step - loss: 0.6593 - val_loss: 0.6615\n",
      "Epoch 58/100\n",
      "19430/19430 [==============================] - 1s 45us/step - loss: 0.6582 - val_loss: 0.6614\n",
      "Epoch 59/100\n",
      "19430/19430 [==============================] - 1s 46us/step - loss: 0.6600 - val_loss: 0.6676\n",
      "Epoch 60/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 0.6566 - val_loss: 0.6622\n",
      "Epoch 61/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.6557 - val_loss: 0.6618\n",
      "Epoch 62/100\n",
      "19430/19430 [==============================] - 1s 39us/step - loss: 0.6570 - val_loss: 0.6532\n",
      "Epoch 63/100\n",
      "19430/19430 [==============================] - 1s 43us/step - loss: 0.6553 - val_loss: 0.6600\n",
      "Epoch 64/100\n",
      "19430/19430 [==============================] - 1s 34us/step - loss: 0.6548 - val_loss: 0.6581\n",
      "Epoch 65/100\n",
      "19430/19430 [==============================] - 1s 34us/step - loss: 0.6537 - val_loss: 0.6541\n",
      "Epoch 66/100\n",
      "19430/19430 [==============================] - 1s 37us/step - loss: 0.6536 - val_loss: 0.6567\n",
      "Epoch 67/100\n",
      "19430/19430 [==============================] - 1s 42us/step - loss: 0.6536 - val_loss: 0.6510\n",
      "Epoch 68/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.6520 - val_loss: 0.6496\n",
      "Epoch 69/100\n",
      "19430/19430 [==============================] - 1s 40us/step - loss: 0.6517 - val_loss: 0.6543\n",
      "Epoch 70/100\n",
      "19430/19430 [==============================] - 1s 37us/step - loss: 0.6506 - val_loss: 0.6510\n",
      "Epoch 71/100\n",
      "19430/19430 [==============================] - 1s 41us/step - loss: 0.6494 - val_loss: 0.6500\n",
      "Epoch 72/100\n",
      "19430/19430 [==============================] - 1s 39us/step - loss: 0.6497 - val_loss: 0.6547\n",
      "Epoch 73/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.6485 - val_loss: 0.6488\n",
      "Epoch 74/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 0.6482 - val_loss: 0.6477\n",
      "Epoch 75/100\n",
      "19430/19430 [==============================] - 1s 38us/step - loss: 0.6480 - val_loss: 0.6491\n",
      "Epoch 76/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 0.6466 - val_loss: 0.6535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 0.6456 - val_loss: 0.6469\n",
      "Epoch 78/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6448 - val_loss: 0.6461\n",
      "Epoch 79/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6441 - val_loss: 0.6426\n",
      "Epoch 80/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6434 - val_loss: 0.6475\n",
      "Epoch 81/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6422 - val_loss: 0.6444\n",
      "Epoch 82/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6422 - val_loss: 0.6405\n",
      "Epoch 83/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6413 - val_loss: 0.6454\n",
      "Epoch 84/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6400 - val_loss: 0.6387\n",
      "Epoch 85/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6393 - val_loss: 0.6423\n",
      "Epoch 86/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6390 - val_loss: 0.6416\n",
      "Epoch 87/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6380 - val_loss: 0.6388\n",
      "Epoch 88/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6372 - val_loss: 0.6392\n",
      "Epoch 89/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6364 - val_loss: 0.6379\n",
      "Epoch 90/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6361 - val_loss: 0.6390\n",
      "Epoch 91/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6341 - val_loss: 0.6381\n",
      "Epoch 92/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6347 - val_loss: 0.6381\n",
      "Epoch 93/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 0.6333 - val_loss: 0.6366\n",
      "Epoch 94/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6322 - val_loss: 0.6304\n",
      "Epoch 95/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 0.6310 - val_loss: 0.6388\n",
      "Epoch 96/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 0.6322 - val_loss: 0.6353\n",
      "Epoch 97/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 0.6310 - val_loss: 0.6353\n",
      "Epoch 98/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 0.6286 - val_loss: 0.6358\n",
      "Epoch 99/100\n",
      "19430/19430 [==============================] - 1s 41us/step - loss: 0.6290 - val_loss: 0.6287\n",
      "Epoch 100/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 0.6284 - val_loss: 0.6326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2d443320>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model2 = Sequential()\n",
    "Model2.add(Dense(50,input_shape = (100,),name = 'inter2'))\n",
    "Model2.add(LeakyReLU(alpha = 0.3))\n",
    "Model2.add(Dense(100,))\n",
    "Model2.add(LeakyReLU(alpha = 0.3))\n",
    "Model2.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "Model2.fit(middle1_out,middle1_out,epochs = 100,batch_size = 64, validation_data = (middle1_out_vld,middle1_out_vld))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle2 = Model(inputs = Model2.input, outputs = Model2.get_layer('inter2').output)\n",
    "middle2_out = middle2.predict(middle1_out)\n",
    "middle2_out_vld = middle2.predict(middle1_out_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  76.06531525,   49.27150345,   52.35367203,  141.67564392,\n",
       "         69.30776215,   85.77253723,   98.19774628,   57.50793457,\n",
       "         37.42586899,  118.96064758,  129.3445282 ,   42.38468933,\n",
       "        128.64471436,   76.76872253,   71.05738831,   50.81737518,\n",
       "         58.40584946,  108.48958588,   50.15361404,  137.61569214,\n",
       "         48.7522049 ,   52.7849884 ,   94.38228607,   61.93455505,\n",
       "         50.61655426,   29.04439926,   56.17163849,   51.87415314,\n",
       "        106.27674103,  128.70887756,   57.56659698,  112.82585144,\n",
       "         84.95211792,   68.45430756,   66.93554688,   83.50508881,\n",
       "         10.15779209,   61.95483017,   41.12686157,   91.71226501,\n",
       "         56.93369675,   41.01479721,   50.22705078,   98.9598465 ,\n",
       "        123.56528473,   63.40103149,   58.94500351,   41.58146667,\n",
       "         88.21643066,   81.37075806], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(middle2_out, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19430 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "19430/19430 [==============================] - 1s 38us/step - loss: 14.3715 - val_loss: 8.7826\n",
      "Epoch 2/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 7.2102 - val_loss: 6.3036\n",
      "Epoch 3/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 5.6342 - val_loss: 5.2209\n",
      "Epoch 4/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 4.8304 - val_loss: 4.5725\n",
      "Epoch 5/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 4.3218 - val_loss: 4.1759\n",
      "Epoch 6/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 3.9586 - val_loss: 3.8464\n",
      "Epoch 7/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 3.7076 - val_loss: 3.6310\n",
      "Epoch 8/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 3.5137 - val_loss: 3.4544\n",
      "Epoch 9/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 3.3791 - val_loss: 3.3420\n",
      "Epoch 10/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 3.2785 - val_loss: 3.2374\n",
      "Epoch 11/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 3.2001 - val_loss: 3.2082\n",
      "Epoch 12/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 3.1420 - val_loss: 3.1182\n",
      "Epoch 13/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 3.0933 - val_loss: 3.0841\n",
      "Epoch 14/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 3.0581 - val_loss: 3.0408\n",
      "Epoch 15/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 3.0288 - val_loss: 2.9983\n",
      "Epoch 16/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 2.9957 - val_loss: 2.9838\n",
      "Epoch 17/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.9751 - val_loss: 2.9400\n",
      "Epoch 18/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.9515 - val_loss: 2.9307\n",
      "Epoch 19/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.9360 - val_loss: 2.9180\n",
      "Epoch 20/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.9186 - val_loss: 2.8992\n",
      "Epoch 21/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.9004 - val_loss: 2.8795\n",
      "Epoch 22/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.8891 - val_loss: 2.8675\n",
      "Epoch 23/100\n",
      "19430/19430 [==============================] - 1s 31us/step - loss: 2.8748 - val_loss: 2.8506\n",
      "Epoch 24/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 2.8647 - val_loss: 2.8320\n",
      "Epoch 25/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.8505 - val_loss: 2.8392\n",
      "Epoch 26/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.8514 - val_loss: 2.8170\n",
      "Epoch 27/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.8343 - val_loss: 2.8158\n",
      "Epoch 28/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.8276 - val_loss: 2.8053\n",
      "Epoch 29/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.8175 - val_loss: 2.8159\n",
      "Epoch 30/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.8119 - val_loss: 2.7835\n",
      "Epoch 31/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.8041 - val_loss: 2.7866\n",
      "Epoch 32/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 2.8004 - val_loss: 2.7774\n",
      "Epoch 33/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.7959 - val_loss: 2.7660\n",
      "Epoch 34/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7905 - val_loss: 2.7635\n",
      "Epoch 35/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7897 - val_loss: 2.7750\n",
      "Epoch 36/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.7863 - val_loss: 2.7697\n",
      "Epoch 37/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7807 - val_loss: 2.7515\n",
      "Epoch 38/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7750 - val_loss: 2.7477\n",
      "Epoch 39/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7735 - val_loss: 2.7337\n",
      "Epoch 40/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7679 - val_loss: 2.7421\n",
      "Epoch 41/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7671 - val_loss: 2.7305\n",
      "Epoch 42/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7647 - val_loss: 2.7410\n",
      "Epoch 43/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7618 - val_loss: 2.7227\n",
      "Epoch 44/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7575 - val_loss: 2.7202\n",
      "Epoch 45/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7561 - val_loss: 2.7236\n",
      "Epoch 46/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7573 - val_loss: 2.7395\n",
      "Epoch 47/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7518 - val_loss: 2.7304\n",
      "Epoch 48/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7505 - val_loss: 2.7161\n",
      "Epoch 49/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7506 - val_loss: 2.7302\n",
      "Epoch 50/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7504 - val_loss: 2.7251\n",
      "Epoch 51/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7487 - val_loss: 2.7298\n",
      "Epoch 52/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7437 - val_loss: 2.7104\n",
      "Epoch 53/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.7409 - val_loss: 2.7224\n",
      "Epoch 54/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.7440 - val_loss: 2.7373\n",
      "Epoch 55/100\n",
      "19430/19430 [==============================] - 0s 26us/step - loss: 2.7411 - val_loss: 2.7167\n",
      "Epoch 56/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 2.7393 - val_loss: 2.7231\n",
      "Epoch 57/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7353 - val_loss: 2.7176\n",
      "Epoch 58/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7370 - val_loss: 2.7172\n",
      "Epoch 59/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7361 - val_loss: 2.6956\n",
      "Epoch 60/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7366 - val_loss: 2.7159\n",
      "Epoch 61/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7329 - val_loss: 2.7073\n",
      "Epoch 62/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7301 - val_loss: 2.7038\n",
      "Epoch 63/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7308 - val_loss: 2.7447\n",
      "Epoch 64/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.7327 - val_loss: 2.7015\n",
      "Epoch 65/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7320 - val_loss: 2.7087\n",
      "Epoch 66/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7320 - val_loss: 2.7050\n",
      "Epoch 67/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7266 - val_loss: 2.7058\n",
      "Epoch 68/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7239 - val_loss: 2.7051\n",
      "Epoch 69/100\n",
      "19430/19430 [==============================] - 0s 26us/step - loss: 2.7261 - val_loss: 2.7003\n",
      "Epoch 70/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7249 - val_loss: 2.6990\n",
      "Epoch 71/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7260 - val_loss: 2.7026\n",
      "Epoch 72/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7221 - val_loss: 2.6975\n",
      "Epoch 73/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7228 - val_loss: 2.7096\n",
      "Epoch 74/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7220 - val_loss: 2.7001\n",
      "Epoch 75/100\n",
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7207 - val_loss: 2.7119\n",
      "Epoch 76/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7217 - val_loss: 2.6855\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19430/19430 [==============================] - 0s 23us/step - loss: 2.7229 - val_loss: 2.7010\n",
      "Epoch 78/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7208 - val_loss: 2.7249\n",
      "Epoch 79/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.7218 - val_loss: 2.7042\n",
      "Epoch 80/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7147 - val_loss: 2.6909\n",
      "Epoch 81/100\n",
      "19430/19430 [==============================] - 0s 22us/step - loss: 2.7184 - val_loss: 2.7058\n",
      "Epoch 82/100\n",
      "19430/19430 [==============================] - 0s 21us/step - loss: 2.7163 - val_loss: 2.6824\n",
      "Epoch 83/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 2.7152 - val_loss: 2.6868\n",
      "Epoch 84/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7172 - val_loss: 2.6864\n",
      "Epoch 85/100\n",
      "19430/19430 [==============================] - 1s 30us/step - loss: 2.7158 - val_loss: 2.6949\n",
      "Epoch 86/100\n",
      "19430/19430 [==============================] - 1s 32us/step - loss: 2.7156 - val_loss: 2.6997\n",
      "Epoch 87/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 2.7154 - val_loss: 2.6938\n",
      "Epoch 88/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 2.7127 - val_loss: 2.6822\n",
      "Epoch 89/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7128 - val_loss: 2.6871\n",
      "Epoch 90/100\n",
      "19430/19430 [==============================] - 1s 28us/step - loss: 2.7116 - val_loss: 2.6811\n",
      "Epoch 91/100\n",
      "19430/19430 [==============================] - 1s 26us/step - loss: 2.7136 - val_loss: 2.6785\n",
      "Epoch 92/100\n",
      "19430/19430 [==============================] - 1s 34us/step - loss: 2.7099 - val_loss: 2.6939\n",
      "Epoch 93/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 2.7109 - val_loss: 2.6854\n",
      "Epoch 94/100\n",
      "19430/19430 [==============================] - 1s 29us/step - loss: 2.7102 - val_loss: 2.7014\n",
      "Epoch 95/100\n",
      "19430/19430 [==============================] - 1s 27us/step - loss: 2.7111 - val_loss: 2.6750\n",
      "Epoch 96/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7077 - val_loss: 2.6781\n",
      "Epoch 97/100\n",
      "19430/19430 [==============================] - 0s 24us/step - loss: 2.7096 - val_loss: 2.6804\n",
      "Epoch 98/100\n",
      "19430/19430 [==============================] - 1s 33us/step - loss: 2.7091 - val_loss: 2.6844\n",
      "Epoch 99/100\n",
      "19430/19430 [==============================] - 0s 25us/step - loss: 2.7079 - val_loss: 2.6866\n",
      "Epoch 100/100\n",
      "19430/19430 [==============================] - 0s 26us/step - loss: 2.7085 - val_loss: 2.6736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2c3f4588>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model3 = Sequential()\n",
    "Model3.add(Dense(20,input_shape = (50,),name = 'inter3'))\n",
    "Model3.add(LeakyReLU(alpha = 0.3))\n",
    "Model3.add(Dense(50,))\n",
    "Model3.add(LeakyReLU(alpha = 0.3))\n",
    "Model3.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
    "Model3.fit(middle2_out,middle2_out,epochs = 100,batch_size = 64, validation_data = (middle2_out_vld,middle2_out_vld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "middle3 = Model(inputs = Model3.input, outputs = Model3.get_layer('inter3').output)\n",
    "middle3_out = middle3.predict(middle2_out)\n",
    "middle3_out_vld = middle3.predict(middle2_out_vld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 135.59967041,   65.82987213,   56.62382126,   30.11065292,\n",
       "         28.74898148,   18.76107788,  106.70642853,   96.98205566,\n",
       "         24.20425034,  127.88632965,   21.56210136,  146.52131653,\n",
       "         18.7825985 ,   91.41913605,   20.55720711,   36.16321564,\n",
       "         14.81036282,   12.38606644,   24.21364784,   14.81716919], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(middle3_out,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(middle3_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19425</th>\n",
       "      <td>15.943581</td>\n",
       "      <td>9.866013</td>\n",
       "      <td>21.122356</td>\n",
       "      <td>-1.156925</td>\n",
       "      <td>-15.380181</td>\n",
       "      <td>-12.714820</td>\n",
       "      <td>4.594036</td>\n",
       "      <td>8.524107</td>\n",
       "      <td>-20.542030</td>\n",
       "      <td>9.037570</td>\n",
       "      <td>-15.814054</td>\n",
       "      <td>10.869353</td>\n",
       "      <td>-27.148956</td>\n",
       "      <td>15.287834</td>\n",
       "      <td>-11.020136</td>\n",
       "      <td>1.348209</td>\n",
       "      <td>-13.199464</td>\n",
       "      <td>-18.299294</td>\n",
       "      <td>-11.007733</td>\n",
       "      <td>-28.701838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19426</th>\n",
       "      <td>7.971561</td>\n",
       "      <td>11.447915</td>\n",
       "      <td>17.890844</td>\n",
       "      <td>-23.836309</td>\n",
       "      <td>-52.130943</td>\n",
       "      <td>-28.175690</td>\n",
       "      <td>11.610044</td>\n",
       "      <td>8.078794</td>\n",
       "      <td>-33.595161</td>\n",
       "      <td>12.370294</td>\n",
       "      <td>-45.403641</td>\n",
       "      <td>13.712380</td>\n",
       "      <td>-35.026283</td>\n",
       "      <td>12.411966</td>\n",
       "      <td>-8.624310</td>\n",
       "      <td>-11.519264</td>\n",
       "      <td>-30.856258</td>\n",
       "      <td>-32.514439</td>\n",
       "      <td>-50.491409</td>\n",
       "      <td>-30.370806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19427</th>\n",
       "      <td>8.696778</td>\n",
       "      <td>15.761197</td>\n",
       "      <td>21.532644</td>\n",
       "      <td>-65.125832</td>\n",
       "      <td>-51.503235</td>\n",
       "      <td>-59.551987</td>\n",
       "      <td>4.164798</td>\n",
       "      <td>15.069073</td>\n",
       "      <td>-28.311832</td>\n",
       "      <td>8.514996</td>\n",
       "      <td>-51.253830</td>\n",
       "      <td>16.239492</td>\n",
       "      <td>-18.043514</td>\n",
       "      <td>3.355330</td>\n",
       "      <td>-40.173748</td>\n",
       "      <td>-23.412256</td>\n",
       "      <td>-38.581284</td>\n",
       "      <td>-35.375664</td>\n",
       "      <td>-57.573151</td>\n",
       "      <td>-46.846222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19428</th>\n",
       "      <td>2.448001</td>\n",
       "      <td>7.151957</td>\n",
       "      <td>5.649827</td>\n",
       "      <td>-3.695565</td>\n",
       "      <td>-6.582603</td>\n",
       "      <td>-2.530823</td>\n",
       "      <td>1.253296</td>\n",
       "      <td>5.311233</td>\n",
       "      <td>-9.669888</td>\n",
       "      <td>6.486143</td>\n",
       "      <td>-8.986037</td>\n",
       "      <td>6.017430</td>\n",
       "      <td>-3.428592</td>\n",
       "      <td>2.726583</td>\n",
       "      <td>-4.636242</td>\n",
       "      <td>-8.341380</td>\n",
       "      <td>-9.649304</td>\n",
       "      <td>-4.928029</td>\n",
       "      <td>-5.482189</td>\n",
       "      <td>-1.434256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19429</th>\n",
       "      <td>10.985883</td>\n",
       "      <td>10.414428</td>\n",
       "      <td>13.659563</td>\n",
       "      <td>-14.063864</td>\n",
       "      <td>-19.012817</td>\n",
       "      <td>-36.729515</td>\n",
       "      <td>6.744277</td>\n",
       "      <td>10.182863</td>\n",
       "      <td>-14.065169</td>\n",
       "      <td>6.892967</td>\n",
       "      <td>-16.966274</td>\n",
       "      <td>7.804955</td>\n",
       "      <td>3.473657</td>\n",
       "      <td>3.786479</td>\n",
       "      <td>-13.157342</td>\n",
       "      <td>-10.071560</td>\n",
       "      <td>-21.367901</td>\n",
       "      <td>-11.526285</td>\n",
       "      <td>-19.210907</td>\n",
       "      <td>-14.197336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1          2          3          4          5   \\\n",
       "19425  15.943581   9.866013  21.122356  -1.156925 -15.380181 -12.714820   \n",
       "19426   7.971561  11.447915  17.890844 -23.836309 -52.130943 -28.175690   \n",
       "19427   8.696778  15.761197  21.532644 -65.125832 -51.503235 -59.551987   \n",
       "19428   2.448001   7.151957   5.649827  -3.695565  -6.582603  -2.530823   \n",
       "19429  10.985883  10.414428  13.659563 -14.063864 -19.012817 -36.729515   \n",
       "\n",
       "              6          7          8          9          10         11  \\\n",
       "19425   4.594036   8.524107 -20.542030   9.037570 -15.814054  10.869353   \n",
       "19426  11.610044   8.078794 -33.595161  12.370294 -45.403641  13.712380   \n",
       "19427   4.164798  15.069073 -28.311832   8.514996 -51.253830  16.239492   \n",
       "19428   1.253296   5.311233  -9.669888   6.486143  -8.986037   6.017430   \n",
       "19429   6.744277  10.182863 -14.065169   6.892967 -16.966274   7.804955   \n",
       "\n",
       "              12         13         14         15         16         17  \\\n",
       "19425 -27.148956  15.287834 -11.020136   1.348209 -13.199464 -18.299294   \n",
       "19426 -35.026283  12.411966  -8.624310 -11.519264 -30.856258 -32.514439   \n",
       "19427 -18.043514   3.355330 -40.173748 -23.412256 -38.581284 -35.375664   \n",
       "19428  -3.428592   2.726583  -4.636242  -8.341380  -9.649304  -4.928029   \n",
       "19429   3.473657   3.786479 -13.157342 -10.071560 -21.367901 -11.526285   \n",
       "\n",
       "              18         19  \n",
       "19425 -11.007733 -28.701838  \n",
       "19426 -50.491409 -30.370806  \n",
       "19427 -57.573151 -46.846222  \n",
       "19428  -5.482189  -1.434256  \n",
       "19429 -19.210907 -14.197336  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19430, 20)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the third layer output for PCA analysis\n",
    "df.to_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/20nodes_output_norm_leakyrelu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#tSNE analysis on 20-nodes output\n",
    "tsne=TSNE(n_components=2, n_iter=300, verbose=1)\n",
    "embedding = tsne.fit_transform(df)\n",
    "\n",
    "vis_x = embedding[:, 0]\n",
    "vis_y = embedding[:, 1]\n",
    "\n",
    "plt.scatter(vis_x, vis_y)\n",
    "plt.savefig('20nodes_tSNE_leakyrelu.pdf',bbox_inches = 'tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#contrastive PCA\n",
    "foreground_data = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/training_leaky_20nodes.csv\",\n",
    "                        index_col = 0)\n",
    "foreground_data.shape\n",
    "\n",
    "background_data = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/testing_leaky_20nodes.csv\",\n",
    "                        index_col = 0)\n",
    "background_data.shape\n",
    "\n",
    "\n",
    "x = np.array([\"1\", \"0\"])\n",
    "label = np.repeat(x, [287, 717], axis=0)\n",
    "\n",
    "mdl = CPCA()\n",
    "projected_data = mdl.fit_transform(foreground_data.values, background_data.values,plot = True, gui = True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl \n",
    "import json \n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_1 = Model1.get_weights()[0]\n",
    "weight_2 = Model2.get_weights()[0]\n",
    "weight_3 = Model3.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 100)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 20)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with open(\"weight_1\",'wb') as f:\n",
    "    pkl.dump(weight_1,f)\n",
    "with open(\"weight_2\",'wb') as f:\n",
    "    pkl.dump(weight_2,f)\n",
    "with open(\"weight_3\",'wb') as f:\n",
    "    pkl.dump(weight_3,f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializer for pre-trained weights; define functions for weight\n",
    "def my_init1(shape, dtype = None):\n",
    "    return weight_1\n",
    "\n",
    "def my_init2(shape,dtype = None):\n",
    "    return weight_2\n",
    "\n",
    "def my_init3(shape,dtype = None):\n",
    "    return weight_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0,input_shape=(359,)))\n",
    "mlp.add(Dense(100,kernel_initializer = my_init1,activity_regularizer=regularizers.l1(10e-7)))\n",
    "mlp.add(LeakyReLU(alpha = 0.3))\n",
    "#mlp.add(Dropout(0.2))\n",
    "mlp.add(Dense(50,kernel_initializer = my_init2,activity_regularizer=regularizers.l1(10e-7)))\n",
    "#mlp.add(LeakyReLU(alpha = 0.3))\n",
    "mlp.add(Dropout(0.2))\n",
    "mlp.add(Dense(20,kernel_initializer = my_init3,activity_regularizer=regularizers.l1(10e-7)))\n",
    "#mlp.add(LeakyReLU(alpha = 0.3))\n",
    "mlp.add(Dense(1))\n",
    "#mlp.add(LeakyReLU(alpha = 0.3))\n",
    "mlp.compile(loss = 'binary_crossentropy',\n",
    "\t\toptimizer = 'adam',\n",
    "\t\tmetrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E001h3k4me3</th>\n",
       "      <th>E002h3k4me3</th>\n",
       "      <th>E003h3k4me3</th>\n",
       "      <th>E004h3k4me3</th>\n",
       "      <th>E005h3k4me3</th>\n",
       "      <th>E006h3k4me3</th>\n",
       "      <th>E007h3k4me3</th>\n",
       "      <th>E008h3k4me3</th>\n",
       "      <th>E009h3k4me3</th>\n",
       "      <th>E010h3k4me3</th>\n",
       "      <th>...</th>\n",
       "      <th>E056tensor</th>\n",
       "      <th>E059tensor</th>\n",
       "      <th>E089tensor</th>\n",
       "      <th>E090tensor</th>\n",
       "      <th>E094tensor</th>\n",
       "      <th>E097tensor</th>\n",
       "      <th>E098tensor</th>\n",
       "      <th>E100tensor</th>\n",
       "      <th>E109tensor</th>\n",
       "      <th>epit.entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000004866</th>\n",
       "      <td>0.296625</td>\n",
       "      <td>0.105981</td>\n",
       "      <td>0.249337</td>\n",
       "      <td>0.184611</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>-0.103241</td>\n",
       "      <td>-0.254451</td>\n",
       "      <td>-0.153236</td>\n",
       "      <td>0.567978</td>\n",
       "      <td>2.079856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525170</td>\n",
       "      <td>1.154039</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>0.718738</td>\n",
       "      <td>0.699267</td>\n",
       "      <td>0.061643</td>\n",
       "      <td>1.580294</td>\n",
       "      <td>2.929246</td>\n",
       "      <td>0.954994</td>\n",
       "      <td>0.658541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000005339</th>\n",
       "      <td>0.664484</td>\n",
       "      <td>0.469639</td>\n",
       "      <td>0.848853</td>\n",
       "      <td>1.325303</td>\n",
       "      <td>0.711186</td>\n",
       "      <td>0.610536</td>\n",
       "      <td>-0.522232</td>\n",
       "      <td>0.906287</td>\n",
       "      <td>0.822634</td>\n",
       "      <td>1.136299</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.499322</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.400658</td>\n",
       "      <td>-0.011319</td>\n",
       "      <td>0.565993</td>\n",
       "      <td>0.335474</td>\n",
       "      <td>0.384709</td>\n",
       "      <td>-0.146969</td>\n",
       "      <td>-0.043804</td>\n",
       "      <td>0.821583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000005513</th>\n",
       "      <td>0.204398</td>\n",
       "      <td>1.045012</td>\n",
       "      <td>0.147311</td>\n",
       "      <td>-0.064777</td>\n",
       "      <td>1.585373</td>\n",
       "      <td>2.315699</td>\n",
       "      <td>-0.443473</td>\n",
       "      <td>0.536371</td>\n",
       "      <td>1.998647</td>\n",
       "      <td>0.870081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411337</td>\n",
       "      <td>0.642630</td>\n",
       "      <td>0.706508</td>\n",
       "      <td>0.171195</td>\n",
       "      <td>0.832542</td>\n",
       "      <td>0.198559</td>\n",
       "      <td>-0.158738</td>\n",
       "      <td>-0.293455</td>\n",
       "      <td>-0.614546</td>\n",
       "      <td>0.555202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000006704</th>\n",
       "      <td>-0.035464</td>\n",
       "      <td>0.227201</td>\n",
       "      <td>0.362198</td>\n",
       "      <td>0.396463</td>\n",
       "      <td>1.144966</td>\n",
       "      <td>1.109917</td>\n",
       "      <td>1.588511</td>\n",
       "      <td>1.286247</td>\n",
       "      <td>0.893187</td>\n",
       "      <td>0.437056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069840</td>\n",
       "      <td>-0.252335</td>\n",
       "      <td>0.298708</td>\n",
       "      <td>0.079938</td>\n",
       "      <td>-0.233652</td>\n",
       "      <td>-0.075272</td>\n",
       "      <td>-0.267428</td>\n",
       "      <td>-0.439942</td>\n",
       "      <td>-0.186490</td>\n",
       "      <td>1.063549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000007168</th>\n",
       "      <td>0.896981</td>\n",
       "      <td>0.367413</td>\n",
       "      <td>0.612749</td>\n",
       "      <td>1.201246</td>\n",
       "      <td>0.086256</td>\n",
       "      <td>0.387711</td>\n",
       "      <td>0.454905</td>\n",
       "      <td>0.145055</td>\n",
       "      <td>1.162038</td>\n",
       "      <td>1.392407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752834</td>\n",
       "      <td>0.898335</td>\n",
       "      <td>0.910407</td>\n",
       "      <td>0.536223</td>\n",
       "      <td>0.965816</td>\n",
       "      <td>1.156964</td>\n",
       "      <td>0.710778</td>\n",
       "      <td>0.878436</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>0.392160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 359 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 E001h3k4me3  E002h3k4me3  E003h3k4me3  E004h3k4me3  \\\n",
       "ENSG00000004866     0.296625     0.105981     0.249337     0.184611   \n",
       "ENSG00000005339     0.664484     0.469639     0.848853     1.325303   \n",
       "ENSG00000005513     0.204398     1.045012     0.147311    -0.064777   \n",
       "ENSG00000006704    -0.035464     0.227201     0.362198     0.396463   \n",
       "ENSG00000007168     0.896981     0.367413     0.612749     1.201246   \n",
       "\n",
       "                 E005h3k4me3  E006h3k4me3  E007h3k4me3  E008h3k4me3  \\\n",
       "ENSG00000004866     0.007758    -0.103241    -0.254451    -0.153236   \n",
       "ENSG00000005339     0.711186     0.610536    -0.522232     0.906287   \n",
       "ENSG00000005513     1.585373     2.315699    -0.443473     0.536371   \n",
       "ENSG00000006704     1.144966     1.109917     1.588511     1.286247   \n",
       "ENSG00000007168     0.086256     0.387711     0.454905     0.145055   \n",
       "\n",
       "                 E009h3k4me3  E010h3k4me3      ...       E056tensor  \\\n",
       "ENSG00000004866     0.567978     2.079856      ...         0.525170   \n",
       "ENSG00000005339     0.822634     1.136299      ...        -0.499322   \n",
       "ENSG00000005513     1.998647     0.870081      ...         0.411337   \n",
       "ENSG00000006704     0.893187     0.437056      ...         0.069840   \n",
       "ENSG00000007168     1.162038     1.392407      ...         0.752834   \n",
       "\n",
       "                 E059tensor  E089tensor  E090tensor  E094tensor  E097tensor  \\\n",
       "ENSG00000004866    1.154039    0.094809    0.718738    0.699267    0.061643   \n",
       "ENSG00000005339    0.003369    0.400658   -0.011319    0.565993    0.335474   \n",
       "ENSG00000005513    0.642630    0.706508    0.171195    0.832542    0.198559   \n",
       "ENSG00000006704   -0.252335    0.298708    0.079938   -0.233652   -0.075272   \n",
       "ENSG00000007168    0.898335    0.910407    0.536223    0.965816    1.156964   \n",
       "\n",
       "                 E098tensor  E100tensor  E109tensor  epit.entropy  \n",
       "ENSG00000004866    1.580294    2.929246    0.954994      0.658541  \n",
       "ENSG00000005339    0.384709   -0.146969   -0.043804      0.821583  \n",
       "ENSG00000005513   -0.158738   -0.293455   -0.614546      0.555202  \n",
       "ENSG00000006704   -0.267428   -0.439942   -0.186490      1.063549  \n",
       "ENSG00000007168    0.710778    0.878436    0.812308      0.392160  \n",
       "\n",
       "[5 rows x 359 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/training_genes_norm.csv\", \n",
    "                      index_col = 0)\n",
    "X_train.shape\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/training_genes_label.csv\",\n",
    "                   index_col = 0)\n",
    "label.shape\n",
    "label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_vld = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/training_genes_validation.csv\",\n",
    "                          index_col = 0)\n",
    "training_label_vld = pd.read_csv(\n",
    "                    \"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/training_genes_validation_label.csv\",\n",
    "                                index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 359)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_vld.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label_vld.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1004 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "1004/1004 [==============================] - 0s 82us/step - loss: 3.7572 - acc: 0.0110 - val_loss: 4.7477 - val_acc: 0.0200\n",
      "Epoch 2/50\n",
      "1004/1004 [==============================] - 0s 60us/step - loss: 5.2553 - acc: 0.0129 - val_loss: 5.3371 - val_acc: 0.0067\n",
      "Epoch 3/50\n",
      "1004/1004 [==============================] - 0s 61us/step - loss: 5.2374 - acc: 0.0070 - val_loss: 5.2855 - val_acc: 0.0067\n",
      "Epoch 4/50\n",
      "1004/1004 [==============================] - 0s 66us/step - loss: 5.2866 - acc: 0.0120 - val_loss: 5.0208 - val_acc: 0.0100\n",
      "Epoch 5/50\n",
      "1004/1004 [==============================] - 0s 78us/step - loss: 5.1245 - acc: 0.0139 - val_loss: 4.8497 - val_acc: 0.0167\n",
      "Epoch 6/50\n",
      "1004/1004 [==============================] - 0s 74us/step - loss: 4.9979 - acc: 0.0090 - val_loss: 4.8468 - val_acc: 0.0133\n",
      "Epoch 7/50\n",
      "1004/1004 [==============================] - 0s 59us/step - loss: 4.8062 - acc: 0.0060 - val_loss: 4.7173 - val_acc: 0.0100\n",
      "Epoch 8/50\n",
      "1004/1004 [==============================] - 0s 59us/step - loss: 4.7956 - acc: 0.0080 - val_loss: 4.7498 - val_acc: 0.0167\n",
      "Epoch 9/50\n",
      "1004/1004 [==============================] - 0s 73us/step - loss: 4.8719 - acc: 0.0129 - val_loss: 4.8994 - val_acc: 0.0100\n",
      "Epoch 10/50\n",
      "1004/1004 [==============================] - 0s 82us/step - loss: 4.8430 - acc: 0.0060 - val_loss: 4.7543 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1004/1004 [==============================] - 0s 70us/step - loss: 4.7470 - acc: 0.0050 - val_loss: 4.6898 - val_acc: 0.0067\n",
      "Epoch 12/50\n",
      "1004/1004 [==============================] - 0s 62us/step - loss: 4.3280 - acc: 0.0139 - val_loss: 4.2212 - val_acc: 0.0167\n",
      "Epoch 13/50\n",
      "1004/1004 [==============================] - 0s 61us/step - loss: 4.2253 - acc: 0.0120 - val_loss: 3.8421 - val_acc: 0.0267\n",
      "Epoch 14/50\n",
      "1004/1004 [==============================] - 0s 68us/step - loss: 3.8227 - acc: 0.0169 - val_loss: 3.5248 - val_acc: 0.0167\n",
      "Epoch 15/50\n",
      "1004/1004 [==============================] - 0s 65us/step - loss: 3.4284 - acc: 0.0090 - val_loss: 3.5805 - val_acc: 0.0133\n",
      "Epoch 16/50\n",
      "1004/1004 [==============================] - 0s 62us/step - loss: 3.3647 - acc: 0.0110 - val_loss: 2.9471 - val_acc: 0.0333\n",
      "Epoch 17/50\n",
      "1004/1004 [==============================] - ETA: 0s - loss: 3.8361 - acc: 0.0000e+0 - 0s 65us/step - loss: 3.1661 - acc: 0.0050 - val_loss: 3.3196 - val_acc: 0.0033\n",
      "Epoch 18/50\n",
      "1004/1004 [==============================] - 0s 80us/step - loss: 3.1730 - acc: 0.0070 - val_loss: 3.3835 - val_acc: 0.0033\n",
      "Epoch 19/50\n",
      "1004/1004 [==============================] - 0s 61us/step - loss: 2.9500 - acc: 0.0080 - val_loss: 3.2778 - val_acc: 0.0033\n",
      "Epoch 20/50\n",
      "1004/1004 [==============================] - 0s 60us/step - loss: 3.0759 - acc: 0.0050 - val_loss: 3.2718 - val_acc: 0.0033\n",
      "Epoch 21/50\n",
      "1004/1004 [==============================] - 0s 59us/step - loss: 3.0796 - acc: 0.0050 - val_loss: 3.2281 - val_acc: 0.0033\n",
      "Epoch 22/50\n",
      "1004/1004 [==============================] - 0s 63us/step - loss: 2.8485 - acc: 0.0090 - val_loss: 3.1195 - val_acc: 0.0033\n",
      "Epoch 23/50\n",
      "1004/1004 [==============================] - 0s 75us/step - loss: 2.8066 - acc: 0.0100 - val_loss: 3.2207 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1004/1004 [==============================] - 0s 73us/step - loss: 3.0767 - acc: 0.0060 - val_loss: 3.0617 - val_acc: 0.0067\n",
      "Epoch 25/50\n",
      "1004/1004 [==============================] - 0s 62us/step - loss: 2.9939 - acc: 0.0060 - val_loss: 2.9387 - val_acc: 0.0067\n",
      "Epoch 26/50\n",
      "1004/1004 [==============================] - 0s 76us/step - loss: 2.8350 - acc: 0.0050 - val_loss: 2.8449 - val_acc: 0.0100\n",
      "Epoch 27/50\n",
      "1004/1004 [==============================] - 0s 89us/step - loss: 2.7862 - acc: 0.0050 - val_loss: 2.6412 - val_acc: 0.0100\n",
      "Epoch 28/50\n",
      "1004/1004 [==============================] - 0s 95us/step - loss: 2.8521 - acc: 0.0090 - val_loss: 2.6317 - val_acc: 0.0067\n",
      "Epoch 29/50\n",
      "1004/1004 [==============================] - 0s 94us/step - loss: 2.8043 - acc: 0.0030 - val_loss: 3.0467 - val_acc: 0.0033\n",
      "Epoch 30/50\n",
      "1004/1004 [==============================] - 0s 90us/step - loss: 2.8667 - acc: 0.0100 - val_loss: 3.1542 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1004/1004 [==============================] - 0s 84us/step - loss: 2.7927 - acc: 0.0040 - val_loss: 2.6730 - val_acc: 0.0067\n",
      "Epoch 32/50\n",
      "1004/1004 [==============================] - 0s 102us/step - loss: 2.7837 - acc: 0.0060 - val_loss: 2.8271 - val_acc: 0.0033\n",
      "Epoch 33/50\n",
      "1004/1004 [==============================] - 0s 87us/step - loss: 2.6656 - acc: 0.0110 - val_loss: 2.8041 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1004/1004 [==============================] - 0s 99us/step - loss: 2.9963 - acc: 0.0129 - val_loss: 3.4607 - val_acc: 0.0133\n",
      "Epoch 35/50\n",
      "1004/1004 [==============================] - 0s 94us/step - loss: 3.5117 - acc: 0.0110 - val_loss: 3.6729 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1004/1004 [==============================] - 0s 88us/step - loss: 3.6225 - acc: 0.0080 - val_loss: 3.6256 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1004/1004 [==============================] - 0s 77us/step - loss: 3.2186 - acc: 0.0080 - val_loss: 3.2019 - val_acc: 0.0067\n",
      "Epoch 38/50\n",
      "1004/1004 [==============================] - 0s 73us/step - loss: 3.0491 - acc: 0.0100 - val_loss: 3.2509 - val_acc: 0.0033\n",
      "Epoch 39/50\n",
      "1004/1004 [==============================] - 0s 70us/step - loss: 2.9554 - acc: 0.0120 - val_loss: 3.1553 - val_acc: 0.0033\n",
      "Epoch 40/50\n",
      "1004/1004 [==============================] - 0s 66us/step - loss: 2.7676 - acc: 0.0110 - val_loss: 2.7749 - val_acc: 0.0167\n",
      "Epoch 41/50\n",
      "1004/1004 [==============================] - 0s 68us/step - loss: 2.6274 - acc: 0.0020 - val_loss: 2.7290 - val_acc: 0.0067\n",
      "Epoch 42/50\n",
      "1004/1004 [==============================] - 0s 68us/step - loss: 2.7844 - acc: 0.0070 - val_loss: 2.7754 - val_acc: 0.0100\n",
      "Epoch 43/50\n",
      "1004/1004 [==============================] - 0s 70us/step - loss: 2.7519 - acc: 0.0090 - val_loss: 2.7736 - val_acc: 0.0033\n",
      "Epoch 44/50\n",
      "1004/1004 [==============================] - 0s 74us/step - loss: 2.7025 - acc: 0.0060 - val_loss: 2.7736 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1004/1004 [==============================] - 0s 82us/step - loss: 2.7509 - acc: 0.0030 - val_loss: 2.7735 - val_acc: 0.0067\n",
      "Epoch 46/50\n",
      "1004/1004 [==============================] - 0s 71us/step - loss: 2.7182 - acc: 0.0060 - val_loss: 2.7757 - val_acc: 0.0100\n",
      "Epoch 47/50\n",
      "1004/1004 [==============================] - 0s 60us/step - loss: 2.7668 - acc: 0.0090 - val_loss: 2.7311 - val_acc: 0.0100\n",
      "Epoch 48/50\n",
      "1004/1004 [==============================] - 0s 59us/step - loss: 2.8028 - acc: 0.0080 - val_loss: 2.6723 - val_acc: 0.0033\n",
      "Epoch 49/50\n",
      "1004/1004 [==============================] - 0s 69us/step - loss: 2.6926 - acc: 0.0050 - val_loss: 2.6747 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1004/1004 [==============================] - 0s 73us/step - loss: 2.8016 - acc: 0.0050 - val_loss: 2.7214 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2f70d898>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train.values,label.values,epochs = 50,batch_size = 64, \n",
    "        validation_data=(training_vld.values, training_label_vld.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-c19de8f7044d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredict_train_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_train_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train.values, label.values)\n",
    "predict_train_Y = model.predict(X_train.values)\n",
    "print (X_train.shape, label.shape, predict_train_Y.shape)\n",
    "label = label.reshape([1004])\n",
    "train_acc = 1.0 * np.sum(predict_train_Y == label) / label.shape[0]\n",
    "print (train_acc)\n",
    "#predict_test_Y = model.predict(self.test_X)\n",
    "#test_acc = 1.0 * np.sum(predict_test_Y == self.test_Y) /self.test_Y.size\n",
    "#print â€˜acc of train and test:â€™, train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/testing_genes_norm.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_out = mlp.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18426, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-5bb38cde3f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msupervised_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "supervised_out.to_cvs(\"/Users/siying/Dropbox/Episcore_working/DeepLearning/data/supervised_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
